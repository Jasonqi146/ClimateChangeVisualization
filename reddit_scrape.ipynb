{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b3b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install praw\n",
    "import praw\n",
    "import pandas as pd\n",
    "from pmaw import PushshiftAPI\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7da9922",
   "metadata": {},
   "source": [
    "Getting posts from pushshift API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd8d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize api\n",
    "api = PushshiftAPI()\n",
    "\n",
    "#set dates\n",
    "before = int(dt.datetime(2022,1,1,0,0).timestamp())\n",
    "after = int(dt.datetime(2021,1,1,0,0).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74b7ab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 4909 submissions from Pushshift\n"
     ]
    }
   ],
   "source": [
    "subreddit=\"climatechange\"\n",
    "limit=100000\n",
    "\n",
    "submissions = api.search_submissions(subreddit=subreddit, limit=limit, before=before, after=after)\n",
    "print(f'Retrieved {len(submissions)} submissions from Pushshift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "027fc4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_df = pd.DataFrame(submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa40b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pushshift_df = submissions_df[[\"author\", \"id\", \"title\", \"full_link\", \n",
    "                               \"score\", \"subreddit\", \"url\", \"num_comments\", \n",
    "                               \"selftext\", \"removed_by_category\", \"created_utc\"]]\n",
    "#what we get from pushshift has discrepency with the real posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09323a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 28076 comments from Pushshift\n"
     ]
    }
   ],
   "source": [
    "# comments = api.search_comments(subreddit=subreddit, limit=limit, before=before, after=after)\n",
    "# print(f'Retrieved {len(comments)} comments from Pushshift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11c05d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_df = pd.DataFrame(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7b1abf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all_awardings', 'associated_award', 'author',\n",
       "       'author_flair_background_color', 'author_flair_css_class',\n",
       "       'author_flair_richtext', 'author_flair_template_id',\n",
       "       'author_flair_text', 'author_flair_text_color', 'author_flair_type',\n",
       "       'author_fullname', 'author_patreon_flair', 'author_premium', 'awarders',\n",
       "       'body', 'collapsed_because_crowd_control', 'collapsed_reason_code',\n",
       "       'comment_type', 'created_utc', 'gildings', 'id', 'is_submitter',\n",
       "       'link_id', 'locked', 'no_follow', 'parent_id', 'permalink',\n",
       "       'retrieved_on', 'score', 'send_replies', 'stickied', 'subreddit',\n",
       "       'subreddit_id', 'top_awarded_type', 'total_awards_received',\n",
       "       'treatment_tags', 'author_cakeday', 'archived', 'body_sha1', 'can_gild',\n",
       "       'collapsed', 'collapsed_reason', 'controversiality', 'distinguished',\n",
       "       'edited', 'gilded', 'retrieved_utc', 'score_hidden',\n",
       "       'subreddit_name_prefixed', 'subreddit_type', 'unrepliable_reason'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comments_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c896012d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>full_link</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>selftext</th>\n",
       "      <th>removed_by_category</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gorgulak</td>\n",
       "      <td>lcn1rf</td>\n",
       "      <td>Donâ€™t climate bet against the house</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>10</td>\n",
       "      <td>climatechange</td>\n",
       "      <td>http://www.realclimate.org/index.php/archives/...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>1612465137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classic-Section-4597</td>\n",
       "      <td>lcfjok</td>\n",
       "      <td>Green Invest</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>1</td>\n",
       "      <td>climatechange</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>0</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>reddit</td>\n",
       "      <td>1612445037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finezysolutions</td>\n",
       "      <td>lcdzhc</td>\n",
       "      <td>Climate change led to increased water salinity...</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>72</td>\n",
       "      <td>climatechange</td>\n",
       "      <td>http://www.zulkernaeen.com/climate-change/mang...</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>1612439263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theworldreviews</td>\n",
       "      <td>lccaka</td>\n",
       "      <td>Ravaging combination: How pandemic, climate cr...</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>9</td>\n",
       "      <td>climatechange</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>0</td>\n",
       "      <td>The past year has presented the world with un...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1612431853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Orpexo</td>\n",
       "      <td>lcc7ue</td>\n",
       "      <td>Discourses of Climate Delay</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>1</td>\n",
       "      <td>climatechange</td>\n",
       "      <td>https://i.redd.it/kxrn15uskff61.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>moderator</td>\n",
       "      <td>1612431510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>lcalmw</td>\n",
       "      <td>Anyone seen full Hansen letter to Boris?</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>1</td>\n",
       "      <td>climatechange</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deleted</td>\n",
       "      <td>1612424657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sirgeogy</td>\n",
       "      <td>lcalkh</td>\n",
       "      <td>Anyone seen full Hansen letter to Boris?</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>1</td>\n",
       "      <td>climatechange</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>0</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>reddit</td>\n",
       "      <td>1612424648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Zarzay</td>\n",
       "      <td>lc8wu0</td>\n",
       "      <td>Please Help With Market Research for Our Susta...</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>1</td>\n",
       "      <td>climatechange</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>0</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>moderator</td>\n",
       "      <td>1612417812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Robby1972</td>\n",
       "      <td>lc7aze</td>\n",
       "      <td>With the economy halted for the whole year (Co...</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>0</td>\n",
       "      <td>climatechange</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>81</td>\n",
       "      <td>We can see the animals recovering their natura...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1612412318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KantKay11</td>\n",
       "      <td>lc214t</td>\n",
       "      <td>So Greta Thunberg supports the protestors in I...</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>7</td>\n",
       "      <td>climatechange</td>\n",
       "      <td>https://www.reddit.com/r/climatechange/comment...</td>\n",
       "      <td>15</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>moderator</td>\n",
       "      <td>1612396365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author      id  \\\n",
       "0              Gorgulak  lcn1rf   \n",
       "1  Classic-Section-4597  lcfjok   \n",
       "2       finezysolutions  lcdzhc   \n",
       "3       theworldreviews  lccaka   \n",
       "4                Orpexo  lcc7ue   \n",
       "5             [deleted]  lcalmw   \n",
       "6              sirgeogy  lcalkh   \n",
       "7                Zarzay  lc8wu0   \n",
       "8             Robby1972  lc7aze   \n",
       "9             KantKay11  lc214t   \n",
       "\n",
       "                                               title  \\\n",
       "0                Donâ€™t climate bet against the house   \n",
       "1                                       Green Invest   \n",
       "2  Climate change led to increased water salinity...   \n",
       "3  Ravaging combination: How pandemic, climate cr...   \n",
       "4                        Discourses of Climate Delay   \n",
       "5           Anyone seen full Hansen letter to Boris?   \n",
       "6           Anyone seen full Hansen letter to Boris?   \n",
       "7  Please Help With Market Research for Our Susta...   \n",
       "8  With the economy halted for the whole year (Co...   \n",
       "9  So Greta Thunberg supports the protestors in I...   \n",
       "\n",
       "                                           full_link  score      subreddit  \\\n",
       "0  https://www.reddit.com/r/climatechange/comment...     10  climatechange   \n",
       "1  https://www.reddit.com/r/climatechange/comment...      1  climatechange   \n",
       "2  https://www.reddit.com/r/climatechange/comment...     72  climatechange   \n",
       "3  https://www.reddit.com/r/climatechange/comment...      9  climatechange   \n",
       "4  https://www.reddit.com/r/climatechange/comment...      1  climatechange   \n",
       "5  https://www.reddit.com/r/climatechange/comment...      1  climatechange   \n",
       "6  https://www.reddit.com/r/climatechange/comment...      1  climatechange   \n",
       "7  https://www.reddit.com/r/climatechange/comment...      1  climatechange   \n",
       "8  https://www.reddit.com/r/climatechange/comment...      0  climatechange   \n",
       "9  https://www.reddit.com/r/climatechange/comment...      7  climatechange   \n",
       "\n",
       "                                                 url  num_comments  \\\n",
       "0  http://www.realclimate.org/index.php/archives/...             0   \n",
       "1  https://www.reddit.com/r/climatechange/comment...             0   \n",
       "2  http://www.zulkernaeen.com/climate-change/mang...             4   \n",
       "3  https://www.reddit.com/r/climatechange/comment...             0   \n",
       "4                https://i.redd.it/kxrn15uskff61.jpg             1   \n",
       "5  https://www.reddit.com/r/climatechange/comment...             0   \n",
       "6  https://www.reddit.com/r/climatechange/comment...             0   \n",
       "7  https://www.reddit.com/r/climatechange/comment...             0   \n",
       "8  https://www.reddit.com/r/climatechange/comment...            81   \n",
       "9  https://www.reddit.com/r/climatechange/comment...            15   \n",
       "\n",
       "                                            selftext removed_by_category  \\\n",
       "0                                                                    NaN   \n",
       "1                                          [removed]              reddit   \n",
       "2                                                                    NaN   \n",
       "3   The past year has presented the world with un...                 NaN   \n",
       "4                                                              moderator   \n",
       "5                                                NaN             deleted   \n",
       "6                                          [removed]              reddit   \n",
       "7                                          [removed]           moderator   \n",
       "8  We can see the animals recovering their natura...                 NaN   \n",
       "9                                          [removed]           moderator   \n",
       "\n",
       "   created_utc  \n",
       "0   1612465137  \n",
       "1   1612445037  \n",
       "2   1612439263  \n",
       "3   1612431853  \n",
       "4   1612431510  \n",
       "5   1612424657  \n",
       "6   1612424648  \n",
       "7   1612417812  \n",
       "8   1612412318  \n",
       "9   1612396365  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pushshift_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc481d7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/climatechange/comments/lcn1rf/dont_climate_bet_against_the_house/\n",
      "https://www.reddit.com/r/climatechange/comments/lcfjok/green_invest/\n",
      "https://www.reddit.com/r/climatechange/comments/lcdzhc/climate_change_led_to_increased_water_salinity/\n",
      "https://www.reddit.com/r/climatechange/comments/lccaka/ravaging_combination_how_pandemic_climate_crisis/\n",
      "https://www.reddit.com/r/climatechange/comments/lcc7ue/discourses_of_climate_delay/\n",
      "https://www.reddit.com/r/climatechange/comments/lcalmw/anyone_seen_full_hansen_letter_to_boris/\n",
      "https://www.reddit.com/r/climatechange/comments/lcalkh/anyone_seen_full_hansen_letter_to_boris/\n",
      "https://www.reddit.com/r/climatechange/comments/lc8wu0/please_help_with_market_research_for_our/\n",
      "https://www.reddit.com/r/climatechange/comments/lc7aze/with_the_economy_halted_for_the_whole_year/\n",
      "https://www.reddit.com/r/climatechange/comments/lc214t/so_greta_thunberg_supports_the_protestors_in/\n"
     ]
    }
   ],
   "source": [
    "for i in pushshift_df['full_link'][0:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0cd093",
   "metadata": {},
   "source": [
    "Run over the raw list and retrieve correct info from Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "929203df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I set this account up earlier with Reddit\n",
    "reddit = praw.Reddit(client_id='3-XsMtgLaC5jxSgvfDrUgQ', \n",
    "                     client_secret='BECT8tnooOvKHtJ5xmJ9bB-MgMQKpg', \n",
    "                     user_agent='ClimateScrapping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd06e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeb7e9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number 0\n",
      "author: None\n",
      "title: Preparing for doom\n",
      "score: 0\n",
      "all comments:\n",
      "................\n",
      "number 1\n",
      "author: MoreTreesFresherAir\n",
      "title: Has any song affected you enough to take any action towards climate change?\n",
      "score: 1\n",
      "all comments:\n",
      "................\n",
      "number 2\n",
      "author: MoreTreesFresherAir\n",
      "title: Do you have a favorite song(s) about climate change? Did those songs encourage you to take any actions to combat climate change? Can you please share what you did?\n",
      "score: 1\n",
      "all comments:\n",
      "................\n",
      "number 3\n",
      "author: leagueofA\n",
      "title: My group and I have just stated and we want to start by trying help stop climate change and to try and help solve other issues in the world. We would like new members in Are group to help us. âœŠðŸ½âœŠðŸ»âœŠðŸ¼âœŠðŸ¾âœŠðŸ¿\n",
      "score: 5\n",
      "all comments:\n",
      "I say this totally without malice or judgment -- why are you starting a new group when there are so many already existing groups out there?\n",
      "I believe the keys to stopping climate change is in understanding the carbon cycle and having diverse energy technology infrastructure.\n",
      "................\n",
      "number 4\n",
      "author: None\n",
      "title: My group and I have just stated and we went you to just are group in trying to help stop climate change and to try and help solve other issues in the world. âœŠðŸ½âœŠðŸ»âœŠðŸ¼âœŠðŸ¾âœŠðŸ¿\n",
      "score: 1\n",
      "all comments:\n",
      "................\n",
      "number 5\n",
      "author: alltwin\n",
      "title: Ice age is coming, see article from IEEE fellow\n",
      "score: 0\n",
      "all comments:\n",
      "[A grand solar minima will not plunge us into a mini-ice age.](https://climatefeedback.org/claimreview/claims-of-a-coming-30-year-mini-ice-age-are-not-supported-by-science-the-sun/)\n",
      "There is already enough extra COÂ² in the atmosphere to cancel the next ice age.\n",
      "LOL. You know it's going to be a serious scientific discussion when it's posted on LumpedIn.\n",
      "Ok, thanks for letting us know.\n",
      "the simple fact is we dont know when its going to happen,\n",
      "hopefully we have a whole wide world better rocket tech when it happens.\n",
      "the safest bet would be to preempt it by eliminating the polar ice altogether\n",
      "................\n",
      "number 6\n",
      "author: None\n",
      "title: I made a short infographic about climate change\n",
      "score: 1\n",
      "all comments:\n",
      "................\n",
      "number 7\n",
      "author: None\n",
      "title: A small brochure about climate change\n",
      "score: 1\n",
      "all comments:\n",
      "................\n",
      "number 8\n",
      "author: None\n",
      "title: I want to care (well I do care) and I want to participate, but I find the topic so boring.\n",
      "score: 1\n",
      "all comments:\n",
      "................\n",
      "number 9\n",
      "author: Ill-Cockroach3107\n",
      "title: Are we too late to stop climate change?!\n",
      "score: 0\n",
      "all comments:\n",
      "Well, yeah. Climate change has already happened.    \n",
      "    \n",
      "But it's a scale. If we make serious changes to what we're doing, and soon, we can reduce the impending damage.     \n",
      "    \n",
      "Just remember, climate change isn't just one single point where everything is doomed. Climate change has been happening for millions of years: it's all about whether we can adapt quickly enough.    \n",
      "    \n",
      "Thinking everything is doomed is just as bad as denying climate change. Vote, make your voice heard, and make personal changes. Channel your worrying into action.\n",
      "Human induced climate change is already happening, so there's no real way to \"stop\" it.\n",
      "\n",
      "With the right expertise and the right amount of action though I think humanity can head off the worst of it. Poorer countries are going to bear the brunt of it though until we can get serious about it.\n",
      "Short answer yes the climate has already changed more natural disasters. longer growing seasons, species migrating....can we stop the planet from becoming uninhabitable I hope so and believe we can\n",
      "Oh! Gotta ask if this website is a credible source please?\n",
      "climate has change since earth got a climate in the first place,\n",
      " there has been no correlation between temperature and co2,\n",
      "http://www.biocab.org/carbon_dioxide_geological_timescale.html\n",
      "and earth is now at the bottom of earth scale of both temperature and co2.\n",
      "the natural state of earth climate is a world without ice,\n",
      "so if it is indeed warming up again, why would you want to stop that ?\n",
      "\n",
      "what was the temperature last night ?\n",
      "would you have survived that night without protection from your environment,\n",
      "in the form of man made heaters and insulation ?\n",
      "We can't even defend our capitol building. What makes you think we have climate under control?\n",
      "................\n",
      "number 10\n",
      "author: None\n",
      "title: 2020 was a terrible year for climate disasters, but there are reasons for hope in 2021\n",
      "score: 1\n",
      "all comments:\n",
      "................\n"
     ]
    }
   ],
   "source": [
    "#test field\n",
    "final_df = []\n",
    "for i in range(len(pushshift_df)):\n",
    "    print(\"number {}\".format(i))\n",
    "    submission = pushshift_df.iloc[i]\n",
    "#     author = submission['author']\n",
    "#     title = submission['title']\n",
    "    link = submission['full_link']\n",
    "#     posts = reddit.subreddit('climatechange').search('author:{} AND title:\\\"{}\\\"'.format(author, title))\n",
    "    post = reddit.submission(url=link)\n",
    "#     print('author:{} AND title:\\\"{}\\\"'.format(author, title))\n",
    "    print(\"author: {}\".format(post.author))\n",
    "    print(\"title: {}\".format(post.title))\n",
    "    print(\"score: {}\".format(post.score))\n",
    "    #api.search_comments()\n",
    "    post.comments.replace_more(limit=0)\n",
    "    print(\"all comments:\")\n",
    "    for top_level_comment in post.comments:\n",
    "        print(top_level_comment.body)\n",
    "        comment.reply_sort = \"new\"\n",
    "        comment.refresh()\n",
    "        replies = comment.replies\n",
    "        print(' ')\n",
    "    final_df.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "    print(\"................\")\n",
    "    if i >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59e99b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is possible to measure CO2 and methane emissions using satellite data.\n",
      "\n",
      "https://en.m.wikipedia.org/wiki/Space-based_measurements_of_carbon_dioxide\n",
      "\n",
      "https://en.m.wikipedia.org/wiki/MethaneSAT\n",
      "num replies: 0\n",
      "xxxxxxxxxx\n",
      "........................................\n",
      "I wouldn't trust China or Russia (or us, for that matter) to self-report emissions accurately. But this stuff is floating around in the air, it's not hard to sample and make pretty good estimates.\n",
      "num replies: 1\n",
      "xxxxxxxxxx\n",
      "dadadadaddyme\n",
      "Nobody is self reporting accurate. Good job on your xenophobia\n",
      "xxxxxxxxx\n",
      "........................................\n",
      "\"Bottom-up\" emission estimate means calculating the emissions. A very simple example:mone cow emits on average X amount of methane. Therefore a farm with 1000 cows emits 1000x methane. It also relies on accurate reporting by the entities involved. You can imagine the uncertainty on that approach is very high. \n",
      "\n",
      "We use satellites to try and measure the emissions \"top-down\" but there are many challenges in measuirng these emissions from space so again, the uncertainty is quite large. That does mean we can monitor emissions unilaterally (e.g. spotting Russian oil/gas pipeline leakes from space which may otherwiswe go unreported).\n",
      "num replies: 0\n",
      "xxxxxxxxxx\n",
      "........................................\n",
      "Check out https://www.climatetrace.org\n",
      "num replies: 0\n",
      "xxxxxxxxxx\n",
      "........................................\n",
      "Our main source is self reporting with a healthy amount of skepticism.\n",
      "num replies: 0\n",
      "xxxxxxxxxx\n",
      "........................................\n",
      "Pretend we can't. So what? We still need to develop the technology to get off fossil fuel dependency to reduce the rate of global emissions.\n",
      "num replies: 1\n",
      "xxxxxxxxxx\n",
      "BoomZhakaLaka\n",
      "well, public support is also part of the requirement, so, useful to understand these arguments and answer them when you can.\n",
      "xxxxxxxxx\n",
      "........................................\n",
      "We depend on each country reporting their emission and they all have an incentive to underreport. That's one of the questions that comes up when we see that CO2 levels kept increasing as before while reported emissions decreased.\n",
      "num replies: 0\n",
      "xxxxxxxxxx\n",
      "........................................\n"
     ]
    }
   ],
   "source": [
    "#working code\n",
    "link = \"https://www.reddit.com/r/climatechange/comments/tkbiir/we_dont_actually_know_chinas_emissions/\"\n",
    "post = reddit.submission(url=link)\n",
    "\n",
    "post.comments.replace_more(limit=0)\n",
    "\n",
    "comment.refresh()\n",
    "for comment in post.comments:\n",
    "    print(comment.body)\n",
    "    comment.reply_sort = \"top\"\n",
    "    comment.refresh()\n",
    "    replies = comment.replies\n",
    "    print(\"num replies: {}\".format(len(replies)))\n",
    "    print(\"xxxxxxxxxx\")\n",
    "    for i in replies:\n",
    "        print(i.author)\n",
    "        print(i.body)\n",
    "        print(\"xxxxxxxxx\")\n",
    "    print(\"........................................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d82c19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_comments(comment):\n",
    "    comment.reply_sort = \"top\"\n",
    "    comment.refresh()\n",
    "    replies = comment.replies\n",
    "    if len(replies) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        lis = []\n",
    "        for r in replies:\n",
    "            attributes = iter_comments(r)\n",
    "            lis.append(attributes)\n",
    "        dic = {}\n",
    "        dic['body'] = comment.body\n",
    "        dic['author'] = comment.author\n",
    "        dic['score'] = comment.score\n",
    "        dic['replies'] = attributes\n",
    "        return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d869f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39f6e895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[],\n",
      " {'author': Redditor(name='ActonofMAM'),\n",
      "  'body': \"I wouldn't trust China or Russia (or us, for that matter) to \"\n",
      "          'self-report emissions accurately. But this stuff is floating around '\n",
      "          \"in the air, it's not hard to sample and make pretty good estimates.\",\n",
      "  'replies': {'author': Redditor(name='dadadadaddyme'),\n",
      "              'body': 'Nobody is self reporting accurate. Good job on your '\n",
      "                      'xenophobia',\n",
      "              'replies': {'author': Redditor(name='Kr155'),\n",
      "                          'body': 'You can acknowledge that China and russia '\n",
      "                                  'are less likely to report accurate numbers '\n",
      "                                  \"since they don't need to answer to a free \"\n",
      "                                  'media without being xenophobic. Some in the '\n",
      "                                  'US are trying really hard to run in the '\n",
      "                                  'same direction. That being said, we can see '\n",
      "                                  \"thier emissions from space so we're not \"\n",
      "                                  'relying on thier data alone.',\n",
      "                          'replies': {'author': Redditor(name='dadadadaddyme'),\n",
      "                                      'body': 'The biggest propaganda outlet '\n",
      "                                              'by far is the us media. The '\n",
      "                                              'biggest contributor to ghgs is '\n",
      "                                              'the us.\\n'\n",
      "                                              '\\n'\n",
      "                                              'Idk what went wrong with '\n",
      "                                              'Americans genz and their '\n",
      "                                              'following western parts but u '\n",
      "                                              'guys need really to pick up a '\n",
      "                                              'history book once in a while',\n",
      "                                      'replies': []}}}},\n",
      " [],\n",
      " [],\n",
      " [],\n",
      " {'author': Redditor(name='iamasatellite'),\n",
      "  'body': \"Pretend we can't. So what? We still need to develop the technology \"\n",
      "          'to get off fossil fuel dependency to reduce the rate of global '\n",
      "          'emissions.',\n",
      "  'replies': []},\n",
      " []]\n"
     ]
    }
   ],
   "source": [
    "link = \"https://www.reddit.com/r/climatechange/comments/tkbiir/we_dont_actually_know_chinas_emissions/\"\n",
    "post = reddit.submission(url=link)\n",
    "\n",
    "post.comments.replace_more(limit=0)\n",
    "\n",
    "lis = []\n",
    "comment.refresh()\n",
    "for comment in post.comments:\n",
    "    lis.append(iter_comments(comment))\n",
    "\n",
    "pprint(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ec26638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e90f678f3b49ceaaa6675e051a335f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#old version without comment forest\n",
    "final_df = []\n",
    "for i in tqdm(range(len(pushshift_df))):\n",
    "    submission = pushshift_df.iloc[i]\n",
    "    link = submission['full_link']\n",
    "    post = reddit.submission(url=link)\n",
    "    post.comments.replace_more(limit=0)\n",
    "    comments = ''\n",
    "    for comment in post.comments:\n",
    "        comments += str(comment.body) + '#^_^#'\n",
    "    final_df.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created, comments])\n",
    "final_df = pd.DataFrame(final_df,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created', 'comments'])\n",
    "final_df.to_csv(\"reddit_posts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "065bfaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a6dc76b18548b59acb8b8c446071df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type Subreddit is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-17c158648cef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'comment_forests.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type Subreddit is not JSON serializable"
     ]
    }
   ],
   "source": [
    "#new version with comment forest\n",
    "final_df = {}\n",
    "for i in tqdm(range(len(pushshift_df))):\n",
    "    \n",
    "    submission = pushshift_df.iloc[i]\n",
    "    link = submission['full_link']\n",
    "    post = reddit.submission(url=link)\n",
    "    \n",
    "    post.comments.replace_more(limit=0)\n",
    "    lis_comments = []\n",
    "    comment.reply_sort = \"top\"\n",
    "    comment.refresh()\n",
    "    \n",
    "    for comment in post.comments:\n",
    "        lis_comments.append(iter_comments(comment))\n",
    "    \n",
    "    final_df['title'] = post.title\n",
    "    final_df['score'] = post.score\n",
    "    final_df['id'] = post.id\n",
    "    final_df['subreddit'] = post.subreddit\n",
    "    final_df['url'] = post.url\n",
    "    final_df['num_comments'] = post.num_comments\n",
    "    final_df['selftext'] = post.selftext\n",
    "    final_df['created'] = post.created\n",
    "    final_df['comments'] = lis_comments\n",
    "    \n",
    "import json\n",
    "with open('comment_forests.json', 'w') as outfile:\n",
    "    json.dump(final_df, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd64e0",
   "metadata": {},
   "source": [
    "Include sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a9552b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "361892f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser\n",
    "result = CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2369737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f763f52c36b4e6185dee5b93edaf84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4909 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#without comment forest\n",
    "final_data = []\n",
    "for i in tqdm(range(len(pushshift_df))):\n",
    "    \n",
    "    submission = pushshift_df.iloc[i]\n",
    "    link = submission['full_link']\n",
    "    post = reddit.submission(url=link)\n",
    "    \n",
    "    post.comments.replace_more(limit=0)\n",
    "    lis_comments = []\n",
    "    post.comments.reply_sort = \"top\"\n",
    "    \n",
    "    for comment in post.comments:\n",
    "        lis_comments.append(comment.body)\n",
    "    final_df = {}\n",
    "    \n",
    "    final_df['title'] = post.title\n",
    "    final_df['score'] = post.score\n",
    "    final_df['id'] = post.id\n",
    "    final_df['subreddit'] = post.subreddit\n",
    "    final_df['url'] = post.url\n",
    "    final_df['num_comments'] = post.num_comments\n",
    "    final_df['selftext'] = post.selftext\n",
    "    final_df['created'] = post.created\n",
    "    final_df['comments'] = lis_comments\n",
    "    \n",
    "#     final_data.append(final_df)\n",
    "#     if i > 100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e9654b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Subreddit is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3239f774de71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2021_data.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type Subreddit is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('2021_data.json', 'w') as outfile:\n",
    "    json.dump(final_df, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c38f705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "print(len(final_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "110638bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who writes this stuff?  \n",
      "\n",
      "No decline in numbers but they have been dying left and right, 25% decline in health but no actual ground truth...so pretty much a completely meaningless article.\n",
      "Interesting\n"
     ]
    }
   ],
   "source": [
    "for i in final_data[2]['comments']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71fc8363",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = final_data[2]['comments'][0]\n",
    "result = nlp.annotate(text,\n",
    "                   properties={\n",
    "                       'annotators': 'sentiment',\n",
    "                       'outputFormat': 'json',\n",
    "                       'timeout': 100000,\n",
    "                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1026b740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 'Who writes this stuff ?': 2 (Sentiment Value) Neutral (Sentiment)\n",
      "1: 'No decline in numbers but they have been dying left and right , 25 % decline in health but no actual ground truth ... so pretty much a completely meaningless article .': 1 (Sentiment Value) Negative (Sentiment)\n"
     ]
    }
   ],
   "source": [
    "for s in result[\"sentences\"]:\n",
    "    print(\"{}: '{}': {} (Sentiment Value) {} (Sentiment)\".format(\n",
    "        s[\"index\"],\n",
    "        \" \".join([t[\"word\"] for t in s[\"tokens\"]]),\n",
    "        s[\"sentimentValue\"], s[\"sentiment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1ff3a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thats not how you do it, if you want to sort out cause and effect, you need to \n",
      "isolate the subject you want to analyze.\n",
      "there is no climate crisis, poverty is on decline, thusly what caused 'truly unprecedented poverty' is purely down to, not cv19, but the knee jerk policies that\n",
      "politicians around the world imposed\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for i in final_data[3]['comments']:\n",
    "    print(i)\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3213c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 'thats not how you do it , if you want to sort out cause and effect , you need to isolate the subject you want to analyze .': 2 (Sentiment Value) Neutral (Sentiment)\n",
      "1: 'there is no climate crisis , poverty is on decline , thusly what caused ` truly unprecedented poverty ' is purely down to , not cv19 , but the knee jerk policies that politicians around the world imposed': 1 (Sentiment Value) Negative (Sentiment)\n"
     ]
    }
   ],
   "source": [
    "text = final_data[3]['comments'][0]\n",
    "result = nlp.annotate(text,\n",
    "                   properties={\n",
    "                       'annotators': 'sentiment',\n",
    "                       'outputFormat': 'json',\n",
    "                       'timeout': 100000,\n",
    "                   })\n",
    "for s in result[\"sentences\"]:\n",
    "    print(\"{}: '{}': {} (Sentiment Value) {} (Sentiment)\".format(\n",
    "        s[\"index\"],\n",
    "        \" \".join([t[\"word\"] for t in s[\"tokens\"]]),\n",
    "        s[\"sentimentValue\"], s[\"sentiment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13fd3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('comment_forests.json', 'w') as outfile:\n",
    "    json.dump(final_df, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
